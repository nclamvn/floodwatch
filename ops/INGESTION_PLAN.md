# ğŸ“… Káº¿ hoáº¡ch Ingest 30 ngÃ y

> **Má»¥c tiÃªu:** Tá»« ZERO data â†’ HUB thÃ´ng tin mÆ°a lÅ© Ä‘áº§y Ä‘á»§ trong 1 thÃ¡ng

---

## ğŸ¯ Tá»•ng quan

### Chiáº¿n lÆ°á»£c
- **Tuáº§n 1-2:** XÃ¢y ná»n mÃ³ng vá»›i nguá»“n tin **chÃ­nh thá»‘ng** (KTTV, bÃ¡o lá»›n)
- **Tuáº§n 3:** Bá»• sung nguá»“n **Ä‘á»‹a phÆ°Æ¡ng** (17 bÃ¡o tá»‰nh miá»n Trung)
- **Tuáº§n 4:** Xá»­ lÃ½ **trÃ¹ng láº·p** + thÃªm **manual entry** cho community

### KPI
| Tuáº§n | Nguá»“n Ingest | Reports/ngÃ y (Æ°á»›c) | Trust Score TB |
|------|--------------|-------------------|----------------|
| 1    | 2-3 nguá»“n    | 10-20             | 0.90+          |
| 2    | 6-8 nguá»“n    | 30-50             | 0.88+          |
| 3    | 15-20 nguá»“n  | 60-100            | 0.85+          |
| 4    | 20-25 nguá»“n  | 80-120            | 0.80+          |

---

## ğŸ“† TUáº¦N 1 (NgÃ y 1-7): Nguá»“n cáº¥p NhÃ  nÆ°á»›c + BÃ¡o lá»›n

### âœ… Má»¥c tiÃªu
- CÃ³ data **tháº­t** tá»« nguá»“n **tin cáº­y nháº¥t**
- Test pipeline ingest â†’ DB â†’ Web â†’ Telegram
- Trust score â‰¥ 0.9

### ğŸ“‹ Nguá»“n triá»ƒn khai

#### 1. KTTV National (kttv.gov.vn)
**Priority:** P0 (Cao nháº¥t)
**Method:** HTML Scrape
**Target pages:**
- `/tin-tuc/canh-bao` - Cáº£nh bÃ¡o thiÃªn tai
- `/du-bao-thoi-tiet` - Dá»± bÃ¡o thá»i tiáº¿t
- `/thong-tin-lu` - ThÃ´ng tin lÅ©

**Script:** `scripts/ingest/kttv_scraper.py`
**Cáº¥u trÃºc:**
```python
def scrape_kttv_alerts():
    """
    - Fetch HTML tá»« /tin-tuc/canh-bao
    - Parse title, content, date, location
    - Extract province tá»« ná»™i dung (regex hoáº·c NER)
    - Geolocate province â†’ lat/lon
    - Create Report vá»›i:
      - type: ALERT
      - source: "kttv_national"
      - trust_score: 0.98
      - categories: ["weather_alert", "flood"]
    """
```

**Output:** 3-5 reports/ngÃ y (trung bÃ¬nh)

---

#### 2. VnExpress - ThiÃªn tai (vnexpress.net)
**Priority:** P0
**Method:** RSS hoáº·c HTML
**RSS Feed:** `https://vnexpress.net/rss/thoi-su.rss` (filter keyword "lÅ©", "mÆ°a", "bÃ£o")
**Fallback:** HTML scrape `/thoi-su/thien-tai`

**Script:** `scripts/ingest/vnexpress_rss.py`
**Cáº¥u trÃºc:**
```python
def scrape_vnexpress_rss():
    """
    - Parse RSS feed
    - Filter items cÃ³ tá»« khÃ³a: lÅ©, mÆ°a, bÃ£o, sáº¡t lá»Ÿ, ngáº­p
    - Extract province tá»« title/description
    - Geolocate
    - Create Report vá»›i:
      - type: ALERT hoáº·c INFO (dá»±a vÃ o keyword)
      - source: "vnexpress_disaster"
      - trust_score: 0.90
      - categories: ["flood", "storm", "landslide"]
    """
```

**Output:** 5-10 reports/ngÃ y

---

#### 3. (Optional) DRVN - ÄÆ°á»ng bá»™ (drvn.gov.vn)
**Priority:** P1
**Method:** HTML Scrape
**Target:** Tin tá»©c vá» ngáº­p Ä‘Æ°á»ng, sáº¡t lá»Ÿ quá»‘c lá»™

**Output:** 2-5 reports/ngÃ y

---

### ğŸ§ª Testing
```bash
# Run manual scrape
python scripts/ingest/kttv_scraper.py

# Check DB
docker compose exec api python3 -c "
from app.database import get_db_context, Report
with get_db_context() as db:
    reports = db.query(Report).filter(Report.source == 'kttv_national').all()
    print(f'Found {len(reports)} reports from KTTV')
"

# Check Web
# Open http://localhost:3002/map
# Should see markers with trust_score â‰¥ 0.9

# Check Telegram
# Subscribe to a province: /subscribe Quáº£ng Trá»‹
# Run scraper again â†’ should receive notification
```

---

## ğŸ“† TUáº¦N 2 (NgÃ y 8-14): BÃ¡o Trung Æ°Æ¡ng + Äá»‹a phÆ°Æ¡ng trá»ng Ä‘iá»ƒm

### âœ… Má»¥c tiÃªu
- Má»Ÿ rá»™ng coverage toÃ n miá»n Trung
- ThÃªm 4-5 nguá»“n bÃ¡o chÃ­nh
- Báº¯t Ä‘áº§u tháº¥y data tá»« nhiá»u tá»‰nh

### ğŸ“‹ Nguá»“n triá»ƒn khai

#### 4. Tuá»•i Tráº» (tuoitre.vn)
**Method:** HTML Scrape `/thoi-su/thien-tai`
**Trust:** 0.90

#### 5. Thanh NiÃªn (thanhnien.vn)
**Method:** HTML Scrape
**Trust:** 0.90

#### 6. BÃ¡o Quáº£ng Trá»‹ (baoquangtri.vn)
**Method:** HTML Scrape
**Trust:** 0.88
**Focus:** LÅ© quÃ©t, sáº¡t lá»Ÿ

#### 7. BÃ¡o Thá»«a ThiÃªn Huáº¿ (baothuathienhue.vn)
**Method:** HTML Scrape
**Trust:** 0.88

#### 8. BÃ¡o ÄÃ  Náºµng (baodanang.vn)
**Method:** HTML Scrape
**Trust:** 0.87
**Focus:** Ngáº­p Ä‘Ã´ thá»‹, giao thÃ´ng

---

### ğŸ”§ Improvements
- **Deduplication (Phase 1):** Check title similarity trÆ°á»›c khi insert
- **Province Extraction:** NÃ¢ng cáº¥p regex â†’ NER model (náº¿u cáº§n)
- **Scheduling:** Cron job má»—i 30 phÃºt cho KTTV, 1h cho bÃ¡o

---

### ğŸ§ª Testing
- Má»—i nguá»“n cháº¡y manual 1 láº§n â†’ verify DB
- Check web map: should see 30-50 reports
- Telegram: Subscribe nhiá»u tá»‰nh â†’ test notifications

---

## ğŸ“† TUáº¦N 3 (NgÃ y 15-21): Má»Ÿ rá»™ng 17 tá»‰nh + Dedup nÃ¢ng cao

### âœ… Má»¥c tiÃªu
- Ingest **Táº¤T Cáº¢** 17 bÃ¡o Ä‘á»‹a phÆ°Æ¡ng trong sources.yaml
- XÃ¢y há»‡ thá»‘ng **deduplication** thÃ´ng minh
- Trust score calibration

### ğŸ“‹ Nguá»“n triá»ƒn khai

Add táº¥t cáº£ bÃ¡o Ä‘á»‹a phÆ°Æ¡ng cÃ²n láº¡i (9-23 trong sources.yaml):
- Quáº£ng Nam, Quáº£ng BÃ¬nh, BÃ¬nh Äá»‹nh, PhÃº YÃªn, KhÃ¡nh HÃ²a
- Gia Lai, Kon Tum, Äáº¯k Láº¯k, Äáº¯k NÃ´ng
- Long An, KiÃªn Giang, An Giang, Cáº§n ThÆ¡, VÄ©nh Long, Nghá»‡ An

**Script:** Generic `scripts/ingest/local_news_scraper.py` vá»›i config tá»« `sources.yaml`

---

### ğŸ§  Deduplication Strategy

#### Problem
CÃ¹ng 1 sá»± kiá»‡n, 5 bÃ¡o Ä‘Æ°a tin â†’ 5 reports trÃ¹ng

#### Solution: Clustering

**Algorithm:**
1. Má»—i report â†’ embedding (title + description)
2. Cosine similarity > 0.8 â†’ cÃ¹ng cluster
3. Merge cluster thÃ nh 1 report "canonical" vá»›i:
   - Title: tá»« nguá»“n trust_score cao nháº¥t
   - Trust score: trung bÃ¬nh cÃ³ trá»ng sá»‘
   - Sources: array of all sources
   - Metadata: list all original reports

**Implementation:**
```python
# scripts/dedup/cluster_reports.py
from sentence_transformers import SentenceTransformer
from sklearn.cluster import DBSCAN

def deduplicate_reports(reports: List[Report]):
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    embeddings = model.encode([r.title + ' ' + (r.description or '') for r in reports])

    clustering = DBSCAN(eps=0.2, min_samples=2, metric='cosine').fit(embeddings)

    for cluster_id in set(clustering.labels_):
        if cluster_id == -1: continue  # Outliers
        cluster_reports = [r for i, r in enumerate(reports) if clustering.labels_[i] == cluster_id]
        merge_cluster(cluster_reports)
```

**Run:** Nightly job (3am) Ä‘á»ƒ merge reports tá»« 24h trÆ°á»›c

---

### ğŸ§ª Testing
- Táº¡o 3 reports vá»›i title gáº§n giá»‘ng â†’ run dedup â†’ verify merged
- Check web: Should NOT see duplicates
- Verify trust score calculation

---

## ğŸ“† TUáº¦N 4 (NgÃ y 22-30): Manual Entry + Community Sources

### âœ… Má»¥c tiÃªu
- XÃ¢y form nháº­p tay cho staff/contributors
- TÃ­ch há»£p manual reports vÃ o map
- (Optional) Thá»­ nghiá»‡m crawl Facebook posts (manual review)

### ğŸ“‹ Tasks

#### 1. Manual Report Form
**File:** `apps/web/app/submit/page.tsx`
**Features:** (xem MANUAL_REPORT_FORM.md)
- Form with title, type, location picker, severity, source URL
- Auto-assign trust_score dá»±a vÃ o source type
- Preview on map trÆ°á»›c khi submit

**API endpoint:** `POST /reports/manual`

---

#### 2. Staff Training
- Viáº¿t hÆ°á»›ng dáº«n cho staff nháº­p reports
- Guidelines: khi nÃ o nháº­p tay, tá»« nguá»“n nÃ o
- Trust score policy

---

#### 3. Community Sources (Manual Review Only)
**NOT automated** - chá»‰ Ä‘á»ƒ staff xem rá»“i nháº­p tay:
- Facebook Groups (Quáº£ng Trá»‹ 24h, etc.)
- OtoSaigon forum
- Zalo groups

**Workflow:**
1. Staff theo dÃµi group
2. Tháº¥y tin quan trá»ng â†’ verify cross-check
3. Nháº­p vÃ o form manual
4. Trust score: 0.6-0.7 (lower than news)

---

### ğŸ§ª Testing
- Staff thá»­ nháº­p 5-10 reports qua form
- Verify reports xuáº¥t hiá»‡n trÃªn map vá»›i trust score Ä‘Ãºng
- Telegram notifications work for manual reports

---

## ğŸ¯ Final Deliverables (End of Week 4)

### âœ… Checklist

- [ ] 20-25 nguá»“n tin Ä‘ang ingest tá»± Ä‘á»™ng
- [ ] 80-120 reports/ngÃ y (trung bÃ¬nh)
- [ ] Deduplication running nightly
- [ ] Manual entry form hoáº¡t Ä‘á»™ng
- [ ] Web map hiá»ƒn thá»‹ Ä‘áº§y Ä‘á»§ vá»›i color coding
- [ ] Telegram bot gá»­i alerts cho subscribers
- [ ] Trust score average â‰¥ 0.80
- [ ] KhÃ´ng cÃ³ duplicates (< 5%)

### ğŸ“Š Metrics Dashboard

Create simple dashboard showing:
- Reports by source (bar chart)
- Reports by province (map heatmap)
- Trust score distribution (histogram)
- Dedup rate (%)
- Telegram subscribers count

---

## ğŸ”„ Maintenance (After Week 4)

### Daily
- Monitor cron jobs (all scrapers running?)
- Check error logs
- Quick scan for duplicates

### Weekly
- Review trust score calibration
- Add/remove sources if needed
- Staff training updates

### Monthly
- Analyze coverage gaps
- User feedback review
- Source reliability audit

---

## ğŸš¨ Contingency Plans

### If a source blocks us
- Implement polite delays (5-10s between requests)
- Rotate User-Agent
- Contact source for API access
- Fallback to manual monitoring

### If dedup fails
- Increase clustering threshold
- Add manual review step
- Implement user reporting for duplicates

### If staff can't keep up with manual entry
- Focus on high-priority provinces only
- Recruit volunteers
- Improve form UX to speed up entry

---

## ğŸ’¡ Future Enhancements (Month 2+)

1. **Image/Video ingestion** - Extract flood photos from news
2. **NLP severity detection** - Auto-assign severity from text
3. **Realtime WebSocket** - Push updates to web without refresh
4. **Mobile app** - React Native for iOS/Android
5. **API for partners** - Chia sáº» data vá»›i cÃ¡c tá»• chá»©c cá»©u trá»£
6. **Historical data** - Archive + analytics

---

**Prepared by:** FloodWatch Dev Team
**Last updated:** 2025-11-18
