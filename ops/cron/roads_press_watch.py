#!/usr/bin/env python3
"""
Road Press Watch - Monitor news sources for road status updates
Scrapes Vietnamese news sites for keywords related to road closures and landslides

Runs every 1 hour
"""

import os
import sys
import requests
import hashlib
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from tenacity import retry, stop_after_attempt, wait_exponential

API_URL = os.getenv("API_BASE_URL", "http://localhost:8000")

# Keywords for road events
KEYWORDS = {
    "CLOSED": ["sáº¡t lá»Ÿ", "chia cáº¯t", "Ä‘Ã³ng cá»­a", "cáº¥m Ä‘Æ°á»ng", "táº¡m dá»«ng lÆ°u thÃ´ng"],
    "RESTRICTED": ["háº¡n cháº¿", "cáº£nh bÃ¡o", "nguy hiá»ƒm", "mÆ°a lá»›n", "tá»‘c Ä‘á»™"],
}

# Common road segments in Central Vietnam
# IMPORTANT: Use correct names! "ÄÃ¨o NhÃ´ng" is in PhÃ¹ Má»¹ district, NOT "ÄÃ¨o PhÃ¹ Má»¹"
ROAD_SEGMENTS = {
    # National highways
    "QL1A": ["Quá»‘c lá»™ 1A", "QL1A", "QL 1A", "quá»‘c lá»™ 1"],
    "QL9": ["Quá»‘c lá»™ 9", "QL9", "QL 9", "Lao Báº£o"],
    "QL14": ["Quá»‘c lá»™ 14", "QL14", "QL 14"],
    "QL49": ["Quá»‘c lá»™ 49", "QL49", "QL 49"],

    # Mountain passes (ÄÃ¨o) - Central Vietnam
    "ÄÃ¨o Háº£i VÃ¢n": ["ÄÃ¨o Háº£i VÃ¢n", "Háº£i VÃ¢n", "Hai Van"],
    "ÄÃ¨o NhÃ´ng": ["ÄÃ¨o NhÃ´ng", "Deo Nhong", "Ä‘Ã¨o NhÃ´ng PhÃ¹ Má»¹"],  # In PhÃ¹ Má»¹ district, BÃ¬nh Äá»‹nh
    "ÄÃ¨o CÃ¹ MÃ´ng": ["ÄÃ¨o CÃ¹ MÃ´ng", "CÃ¹ MÃ´ng", "Cu Mong"],
    "ÄÃ¨o Cáº£": ["ÄÃ¨o Cáº£", "Deo Ca", "Ä‘Ã¨o Cáº£"],
    "ÄÃ¨o LÃ² Xo": ["ÄÃ¨o LÃ² Xo", "LÃ² Xo", "Lo Xo"],
    "ÄÃ¨o Ngang": ["ÄÃ¨o Ngang", "Ngang", "HoÃ nh SÆ¡n"],
    "ÄÃ¨o PhÆ°á»›c TÆ°á»£ng": ["ÄÃ¨o PhÆ°á»›c TÆ°á»£ng", "PhÆ°á»›c TÆ°á»£ng", "Phuoc Tuong"],
    "ÄÃ¨o PhÃº Gia": ["ÄÃ¨o PhÃº Gia", "PhÃº Gia", "Phu Gia"],

    # Bridges (Cáº§u)
    "Cáº§u Tháº¡ch HÃ£n": ["Cáº§u Tháº¡ch HÃ£n", "Tháº¡ch HÃ£n"],
    "Cáº§u Hiá»n LÆ°Æ¡ng": ["Cáº§u Hiá»n LÆ°Æ¡ng", "Hiá»n LÆ°Æ¡ng", "Báº¿n Háº£i"],
}

# Province mapping
PROVINCES = {
    "Quáº£ng BÃ¬nh": ["Quáº£ng BÃ¬nh", "Quang Binh", "Äá»“ng Há»›i"],
    "Quáº£ng Trá»‹": ["Quáº£ng Trá»‹", "Quang Tri", "ÄÃ´ng HÃ "],
    "Thá»«a ThiÃªn Huáº¿": ["Thá»«a ThiÃªn Huáº¿", "Thua Thien Hue", "Huáº¿", "Hue"],
    "ÄÃ  Náºµng": ["ÄÃ  Náºµng", "Da Nang"],
    "Quáº£ng Nam": ["Quáº£ng Nam", "Quang Nam", "Tam Ká»³"],
    "Quáº£ng NgÃ£i": ["Quáº£ng NgÃ£i", "Quang Ngai"],
}

# News sources (simplified - real implementation would have specific selectors per site)
NEWS_SOURCES = [
    {
        "name": "VnExpress",
        "url": "https://vnexpress.net/thoi-su",
        "enabled": False  # Disabled for MVP - requires specific selectors
    },
    {
        "name": "Thanh Nien",
        "url": "https://thanhnien.vn/thoi-su/",
        "enabled": False  # Disabled for MVP
    },
]


class RoadPressWatcher:
    """Monitor news for road status updates"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.ingested_hashes = self._load_ingested_hashes()

    def _load_ingested_hashes(self) -> set:
        """Load previously ingested article hashes to avoid duplicates"""
        hash_file = "/tmp/roads_press_hashes.json"
        try:
            if os.path.exists(hash_file):
                with open(hash_file, 'r') as f:
                    data = json.load(f)
                    # Keep only hashes from last 24 hours
                    cutoff = (datetime.now() - timedelta(hours=24)).isoformat()
                    return set([h for h, ts in data.items() if ts > cutoff])
        except:
            pass
        return set()

    def _save_ingested_hashes(self):
        """Save ingested hashes with timestamps"""
        hash_file = "/tmp/roads_press_hashes.json"
        try:
            data = {h: datetime.now().isoformat() for h in self.ingested_hashes}
            with open(hash_file, 'w') as f:
                json.dump(data, f)
        except Exception as e:
            print(f"âš ï¸  Could not save hashes: {e}")

    def _compute_hash(self, segment: str, province: str, date_bucket: str) -> str:
        """Compute hash for idempotency (segment + province + 6h bucket)"""
        key = f"{segment.lower()}|{province.lower()}|{date_bucket}"
        return hashlib.sha1(key.encode()).hexdigest()

    def _extract_segment_name(self, text: str) -> Optional[str]:
        """Extract road segment name from text"""
        text_lower = text.lower()
        for segment, aliases in ROAD_SEGMENTS.items():
            for alias in aliases:
                if alias.lower() in text_lower:
                    return segment
        return None

    def _extract_province(self, text: str) -> Optional[str]:
        """Extract province from text"""
        text_lower = text.lower()
        for province, aliases in PROVINCES.items():
            for alias in aliases:
                if alias.lower() in text_lower:
                    return province
        return None

    def _determine_status(self, text: str) -> str:
        """Determine road status from text"""
        text_lower = text.lower()

        # Check for CLOSED keywords
        for keyword in KEYWORDS["CLOSED"]:
            if keyword in text_lower:
                return "CLOSED"

        # Check for RESTRICTED keywords
        for keyword in KEYWORDS["RESTRICTED"]:
            if keyword in text_lower:
                return "RESTRICTED"

        return "RESTRICTED"  # Default to RESTRICTED if keywords found

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=4)
    )
    def _fetch_url(self, url: str, timeout: int = 8) -> Optional[str]:
        """Fetch URL with retry"""
        try:
            response = self.session.get(url, timeout=timeout)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ Error fetching {url}: {e}")
            return None

    def scrape_mock_events(self) -> List[Dict]:
        """
        Generate mock road events for testing
        In production, this would scrape real news sites
        """
        import random

        mock_events = []
        segments = list(ROAD_SEGMENTS.keys())
        provinces = list(PROVINCES.keys())

        # Generate 1-2 random events
        for _ in range(random.randint(1, 2)):
            segment = random.choice(segments)
            province = random.choice(provinces)
            status = random.choice(["CLOSED", "RESTRICTED"])

            # Generate date bucket (6h intervals)
            now = datetime.now()
            bucket = now.replace(hour=(now.hour // 6) * 6, minute=0, second=0).isoformat()

            # Compute hash for idempotency
            event_hash = self._compute_hash(segment, province, bucket)

            # Skip if already ingested
            if event_hash in self.ingested_hashes:
                continue

            reason = "Sáº¡t lá»Ÿ nghiÃªm trá»ng" if status == "CLOSED" else "MÆ°a lá»›n, háº¡n cháº¿ tá»‘c Ä‘á»™"

            mock_events.append({
                "segment_name": segment,
                "status": status,
                "reason": reason,
                "province": province,
                "last_verified": now.isoformat(),
                "source": "PRESS",
                "hash": event_hash
            })

        return mock_events

    def ingest_events(self, events: List[Dict]) -> Dict:
        """Send events to API"""
        if not events:
            return {"status": "success", "ingested": 0}

        try:
            # For each event, call API
            ingested = 0
            for event in events:
                event_copy = event.copy()
                event_hash = event_copy.pop("hash")

                # POST to /ingest/road-event endpoint
                response = requests.post(
                    f"{API_URL}/ingest/road-event",
                    json=event_copy,
                    timeout=10
                )

                if response.status_code in [200, 201]:
                    self.ingested_hashes.add(event_hash)
                    ingested += 1
                else:
                    print(f"âš ï¸  API returned {response.status_code}: {response.text}")

            self._save_ingested_hashes()

            return {
                "status": "success",
                "ingested": ingested
            }
        except requests.exceptions.RequestException as e:
            print(f"âŒ Error ingesting events: {e}")
            return {"status": "error", "message": str(e)}

    def run(self):
        """Main run method"""
        print(f"ğŸ”„ [{datetime.now().isoformat()}] Starting road press watch...")

        # For MVP, use mock events
        # In production, would scrape real news sites
        events = self.scrape_mock_events()

        if not events:
            print("â„¹ï¸  No new road events found")
            return

        print(f"ğŸ“Š Found {len(events)} new road events")

        # Log events for debugging
        for event in events:
            print(f"   â€¢ {event['segment_name']} ({event['province']}): {event['status']}")

        # Ingest events to API
        result = self.ingest_events(events)

        if result.get("status") == "success":
            print(f"âœ… Successfully ingested {result.get('ingested', 0)} road events")
        else:
            print(f"âŒ Failed to ingest: {result}")


def main():
    watcher = RoadPressWatcher()
    watcher.run()


if __name__ == "__main__":
    main()
